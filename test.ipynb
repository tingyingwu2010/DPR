{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "# import multiprocessing as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "# from lib.vrp_helper_legacy import random_vrptw\n",
    "from lib.vrp_helper_0 import random_vrptw\n",
    "from lib.vrp_helper_0 import check_vrp_route_validity, check_vrp_routes_validity\n",
    "from lib.vrp_helper_1 import get_distMat, get_inbalance_distMat, get_solDist\n",
    "from lib.vrp_helper_1 import parse_vrp_question, parse_vrp_answer, getQADict\n",
    "from lib.vrp_helper_env import build_edge_index_routes, build_edge_index_near, vrp_env_sisr\n",
    "from lib.ppo_1 import Memory, Agent\n",
    "\n",
    "import os.path\n",
    "import subprocess\n",
    "from subprocess import STDOUT,PIPE\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# torchvision is not compatible with pillow 7.0.0\n",
    "# use conda install pillow=6.1 to downgrade pillow\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, softmax\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "############################################################\n",
    "# 与 Java 交换数据相关的函数\n",
    "\n",
    "def write_data_file(data, path=\"data.txt\"):\n",
    "    lines = []\n",
    "    for i in range(data.shape[0]):\n",
    "        lines.append(\" \".join([str(x) for x in data[i]])+\"\\n\")\n",
    "    with open(path, \"w+\") as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "            \n",
    "def write_distMat_file(dist_matrices, path):\n",
    "    \n",
    "    def get_dist_matrix_str(dist_matrix):\n",
    "        dist_matrix_str = []\n",
    "        for i in range(len(dist_matrix)):\n",
    "            dist_matrix_str.append(\",\".join([str(x) for x in np.round(dist_matrix[i],4)]))\n",
    "        result = \";\".join(dist_matrix_str)\n",
    "        return result\n",
    "    \n",
    "    lines = []\n",
    "    for i in range(len(dist_matrices)):\n",
    "        lines.append(get_dist_matrix_str(dist_matrices[i])+\"\\n\")\n",
    "    with open(path, \"w+\") as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "\n",
    "def write_states_file(batch_caps, batch_ruins, batch_routes, path):\n",
    "    lines = []\n",
    "    for i in range(len(batch_caps)):\n",
    "        content = [str(batch_caps[i]), \",\".join([str(x) for x in batch_ruins[i]])]\n",
    "        routes_str_list = []\n",
    "        for r in batch_routes[i]:\n",
    "            routes_str_list.append(\",\".join([str(x) for x in r]))\n",
    "        content.append(\";\".join(routes_str_list))\n",
    "        lines.append(\":\".join(content)+\"\\n\")\n",
    "    with open(path, \"w+\") as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "\n",
    "def compile_java(java_file):\n",
    "    subprocess.check_call(['javac', java_file])\n",
    "\n",
    "def execute_java(java_file, stdin):\n",
    "    java_class,ext = os.path.splitext(java_file)\n",
    "    cmd = ['java', java_class]+stdin\n",
    "    proc = subprocess.Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=STDOUT)\n",
    "    stdout,stderr = proc.communicate(\"\".encode())\n",
    "    return stdout.decode()\n",
    "\n",
    "def parse_output(_output):\n",
    "    dist, routes = _output.split(\":\")\n",
    "    dist = float(dist)\n",
    "    routes = [[int(y) for y in x.split(\",\")] for x in routes.split(\";\")]\n",
    "    return dist, routes\n",
    "\n",
    "def vrp_java(java_file, stdin):\n",
    "    \n",
    "    def compile_java(java_file):\n",
    "        subprocess.check_call(['javac', java_file])\n",
    "    \n",
    "    def execute_java(java_file, stdin):\n",
    "        java_class,ext = os.path.splitext(java_file)\n",
    "        cmd = ['java', java_class]+stdin\n",
    "        proc = subprocess.Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=STDOUT)\n",
    "        stdout,stderr = proc.communicate(\"\".encode())\n",
    "        return stdout.decode()\n",
    "    \n",
    "    def parse_output(_output):\n",
    "        dist, routes = _output.split(\":\")\n",
    "        dist = float(dist)\n",
    "        routes = [[int(y) for y in x.split(\",\")] for x in routes.split(\";\")]\n",
    "        return dist, routes\n",
    "    \n",
    "    compile_java(java_file)\n",
    "    _output = execute_java(java_file, stdin)\n",
    "    return parse_output(_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters and compile the java files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:1\"\n",
    "starting_iter = 0\n",
    "n_iter = 150\n",
    "n_iter_max = n_iter\n",
    "n_batch = 50\n",
    "n_epoch = 2\n",
    "n_anchors = 2\n",
    "\n",
    "lr = 2e-5\n",
    "betas = (0.9, 0.999)\n",
    "K_epochs = 1\n",
    "eps_clip = 0.2\n",
    "train_batch_size = n_batch\n",
    "\n",
    "gamma = 0.99\n",
    "embedding_dim = 11\n",
    "node_dim = 128\n",
    "critic_dims = 256\n",
    "\n",
    "# 模拟退火\n",
    "init_T = 100.0\n",
    "final_T = 1.0\n",
    "alpha_T = (final_T/init_T)**(1.0/n_iter)\n",
    "\n",
    "model_folder = './models/ppo_1'\n",
    "save_path = model_folder+'/ppo.pth'\n",
    "\n",
    "data_root = \"tmp_datas\"\n",
    "state_path = \"states.txt\"\n",
    "distMat_path = \"distMat.txt\"\n",
    "try:\n",
    "    os.mkdir(data_root)\n",
    "except FileExistsError:\n",
    "    shutil.rmtree(data_root)\n",
    "    os.mkdir(data_root)\n",
    "\n",
    "java_solver_rand = 'java_solverRAND.java'\n",
    "java_solver_sisr2 = 'java_solverSISR_revised.java'\n",
    "java_steper = 'java_recreate.java'\n",
    "\n",
    "try:\n",
    "    os.mkdir(model_folder)\n",
    "    print(\"Model folder created.\")\n",
    "except FileExistsError:\n",
    "    print(\"Model folder already exists.\")\n",
    "_=[os.remove(filename) for filename in os.listdir() if \".class\" in filename] # 清空目录下的 Java 编译文件\n",
    "\n",
    "compile_java(java_solver_rand)\n",
    "compile_java(java_solver_sisr2)\n",
    "compile_java(java_steper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA200 = getQADict(\"data/homberger_200_customer_instances\", \"data/solutions_200\")\n",
    "q200_paths = sorted(list(QA200.keys()))\n",
    "q200 = [parse_vrp_question(q) for q in q200_paths]\n",
    "a200 = [parse_vrp_answer(QA200[q]) for q in q200_paths]\n",
    "sol200 = np.mean([get_solDist(get_distMat(q200[i][1]), a200[i]) for i in range(len(a200))])\n",
    "\n",
    "solomon_paths = [\"data/solomon/\"+x for x in os.listdir(\"data/solomon\") if x[0]!='.']\n",
    "q100 = [parse_vrp_question(q) for q in solomon_paths]\n",
    "q50 = [(x[0],x[1][:51]) for x in q100]\n",
    "q25 = [(x[0],x[1][:26]) for x in q100]\n",
    "\n",
    "dist100 = []\n",
    "dist50 = []\n",
    "dist25 = []\n",
    "with open(\"data/solution_solomon.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        n = line.strip().split(\",\")[0].split('.')[1]\n",
    "        v = float(line.strip().split(\",\")[2])\n",
    "        if int(n)==25:\n",
    "            dist25.append(v)\n",
    "        if int(n)==50:\n",
    "            dist50.append(v)\n",
    "        if int(n)==100:\n",
    "            dist100.append(v)\n",
    "sol25 = np.mean(dist25)\n",
    "sol50 = np.mean(dist50)\n",
    "sol100 = np.mean(dist100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "agent = Agent(lr, betas, gamma, K_epochs, eps_clip, train_batch_size,\n",
    "              embedding_dim, node_dim, critic_dims, n_anchors, device=device)\n",
    "agent.load(save_path)\n",
    "\n",
    "for n_nodes in [25,50,100,200]:\n",
    "    if n_nodes==25:\n",
    "        problems = q25\n",
    "        sol_dist = sol25\n",
    "    elif n_nodes==50:\n",
    "        problems = q50\n",
    "        sol_dist = sol50\n",
    "    elif n_nodes==100:\n",
    "        problems = q100\n",
    "        sol_dist = sol100\n",
    "    else:\n",
    "        problems = q200\n",
    "        sol_dist = sol200\n",
    "    history_init_dists = []\n",
    "    history_rand_dists = []\n",
    "    history_sisr2_dists = []\n",
    "    history_sisrRL_dists = []\n",
    "    cost_time = 0.0\n",
    "    n_batch = len(problems)\n",
    "    for i_epoch in range(1):\n",
    "        n_ruins_d = max(n_nodes**0.5, 1)*1.2\n",
    "        n_ruins = int(np.ceil(n_ruins_d/n_anchors))\n",
    "        n_ruins_total = int(n_ruins*n_anchors)\n",
    "        print(\"Epoch:\", i_epoch, \"-- # nodes =\", n_nodes)\n",
    "\n",
    "        ######################################################################\n",
    "        # 生成问题\n",
    "        envs = []\n",
    "        for i_batch in range(n_batch):\n",
    "            cap, data = problems[n_batch*i_epoch+i_batch]\n",
    "            distMat = get_distMat(data)\n",
    "\n",
    "            data_path = data_root+\"/data_\"+str(i_batch)+\".txt\"\n",
    "            write_data_file(data, path=data_path)\n",
    "\n",
    "            args = [os.path.abspath(os.getcwd())+\"/\"+data_path, str(cap),\n",
    "                    str(starting_iter), str(n_ruins_total)]\n",
    "            _output = execute_java(java_solver_rand, args)\n",
    "            dist, routes = parse_output(_output)\n",
    "            env = vrp_env_sisr(cap, data, distMat, routes, dist, init_T, alpha_T)\n",
    "            envs.append(env)\n",
    "\n",
    "        write_distMat_file([env.distMat for env in envs], path=distMat_path)\n",
    "\n",
    "        edge_index_n = build_edge_index_near([env.distMat for env in envs], 10)\n",
    "\n",
    "        ######################################################################\n",
    "        # 做 benchmark\n",
    "        benchmarks_rand = []\n",
    "        benchmarks_sisr2 = []\n",
    "        for i_batch in range(n_batch):\n",
    "            # 使用java求解最多前 n_benchs 个 benchmark (节约时间)\n",
    "            data_path = data_root+\"/data_\"+str(i_batch)+\".txt\"\n",
    "            args = [os.path.abspath(os.getcwd())+\"/\"+data_path,\n",
    "                    str(envs[i_batch].cap),\n",
    "                    str(starting_iter+n_iter), str(n_ruins_total)]\n",
    "            _output = execute_java(java_solver_rand, args)\n",
    "            dist, routes = parse_output(_output)\n",
    "            benchmarks_rand.append(dist)\n",
    "\n",
    "        write_states_file([env.cap for env in envs],\n",
    "                          [[] for _ in range(len(envs))],\n",
    "                          [env.curr_routes for env in envs],\n",
    "                          state_path)\n",
    "        args = [os.path.abspath(os.getcwd())+\"/\"+data_root+\"/\",\n",
    "                os.path.abspath(os.getcwd())+\"/\"+state_path,\n",
    "                os.path.abspath(os.getcwd())+\"/\"+distMat_path,\n",
    "                str(n_iter), str(n_ruins), str(n_anchors)]\n",
    "        _outputs = execute_java(java_solver_sisr2, args)\n",
    "        for _output in [x for x in _outputs.split(\"\\n\") if len(x)>0]:\n",
    "            dist, routes = parse_output(_output)\n",
    "            benchmarks_sisr2.append(dist)\n",
    "\n",
    "        print(\"Mean init distance =\", np.mean([env.curr_dist for env in envs]))\n",
    "        print(\"Mean RAND operator =\", np.mean(benchmarks_rand))\n",
    "        print(\"Mean SISR operator =\", np.mean(benchmarks_sisr2))\n",
    "        for d in [env.curr_dist for env in envs]:\n",
    "            history_init_dists.append(d)\n",
    "        for d in benchmarks_rand:\n",
    "            history_rand_dists.append(d)\n",
    "        for d in benchmarks_sisr2:\n",
    "            history_sisr2_dists.append(d)\n",
    "\n",
    "        time.sleep(1)\n",
    "        ######################################################################\n",
    "        # 开始迭代\n",
    "        start_time = time.time()\n",
    "        for i_iter in trange(n_iter):\n",
    "            edge_index_r0 = build_edge_index_routes([env.curr_routes for env in envs], inverse=False)\n",
    "            edge_index_r1 = build_edge_index_routes([env.curr_routes for env in envs], inverse=True)\n",
    "\n",
    "            _es = [env.get_embedding() for env in envs]\n",
    "            _es = np.array(_es)\n",
    "            if i_iter == 0:\n",
    "                input_ = np.zeros([n_batch,n_ruins_total,embedding_dim]).astype(np.float32)\n",
    "                input_ = torch.from_numpy(input_).to(torch.device(device))\n",
    "                h = np.zeros([n_batch,node_dim]).astype(np.float32)\n",
    "                h = torch.from_numpy(h).to(torch.device(device))\n",
    "            else:\n",
    "                input_ = (np.array([_es[i][ruins[i]] for i in range(n_batch)])* bs).astype(np.float32) \n",
    "                input_ = torch.from_numpy(input_).to(torch.device(device))\n",
    "                h = h_\n",
    "\n",
    "            data_list = []\n",
    "            for i_batch in range(n_batch):\n",
    "                data = Data(x=torch.from_numpy(_es[i_batch]).float().to(torch.device(device)),\n",
    "                            edge_index_r0=torch.from_numpy(edge_index_r0[i_batch]).to(torch.device(device)),\n",
    "                            edge_index_r1=torch.from_numpy(edge_index_r1[i_batch]).to(torch.device(device)),\n",
    "                            edge_index_n=torch.from_numpy(edge_index_n[i_batch]).to(torch.device(device)),\n",
    "                            state_=h[i_batch].detach().to(torch.device(device)),\n",
    "                            input_=input_[i_batch].detach().to(torch.device(device)))\n",
    "                data_list.append(data)\n",
    "            loader = DataLoader(data_list,batch_size=n_batch,shuffle=False)\n",
    "\n",
    "            tmp = agent.policy_old.act(list(loader)[0], n_nodes+1)\n",
    "            h_, action_node, action_sisr, dist_p, dist_s = tmp\n",
    "\n",
    "            indices = torch.arange(action_node.size(0)).repeat(n_anchors)\n",
    "            indices = indices.reshape([-1,action_node.size(0)]).transpose(0,1)\n",
    "            coeff = action_sisr[indices, action_node].cpu().detach().numpy()\n",
    "            sisr_logs = dist_s.log_prob(action_sisr)[indices, action_node].detach()\n",
    "            prob_logs = [dist_p.log_prob(action_node[:,i]).cpu().detach().numpy() for i in range(n_anchors)]\n",
    "            prob_logs = torch.Tensor(prob_logs).to(torch.device(device))\n",
    "            prob_logs = prob_logs.transpose(0,1)\n",
    "            log_probs = prob_logs + sisr_logs\n",
    "            log_probs = torch.sum(log_probs, 1)\n",
    "            action_node = action_node.detach()\n",
    "\n",
    "            ruins = []\n",
    "            for i_batch in range(n_batch):\n",
    "                tmp_ruins = []\n",
    "                for i_a in range(n_anchors):\n",
    "                    tmp = envs[i_batch].get_ruins(action_node[i_batch][i_a]+1, coeff[i_batch][i_a],\n",
    "                                                  n_ruins, ruined=tmp_ruins)\n",
    "                    tmp_ruins.extend(tmp)\n",
    "                ruins.append(tmp_ruins)\n",
    "\n",
    "            write_states_file([env.cap for env in envs], ruins, [env.curr_routes for env in envs], state_path)\n",
    "            new_routes = []\n",
    "            new_dists = []\n",
    "            args = [os.path.abspath(os.getcwd())+\"/\"+data_root+\"/\",\n",
    "                    os.path.abspath(os.getcwd())+\"/\"+state_path,\n",
    "                    os.path.abspath(os.getcwd())+\"/\"+distMat_path]\n",
    "            _outputs = execute_java(java_steper, args)\n",
    "            for _output in [x for x in _outputs.split(\"\\n\") if len(x)>0]:\n",
    "                dist, routes = parse_output(_output)\n",
    "                new_routes.append(routes)\n",
    "                new_dists.append(dist)\n",
    "\n",
    "            bs = []\n",
    "            for i_batch in range(n_batch):\n",
    "                b, reward = envs[i_batch].sim_annealing(new_dists[i_batch], new_routes[i_batch])\n",
    "                bs.append(float(b))\n",
    "            bs = np.array(bs).reshape([-1,1,1])\n",
    "\n",
    "        ######################################################################\n",
    "        cost_time += time.time()-start_time\n",
    "        print(\"Mean RL operator\", np.mean([env.best_dist for env in envs]))\n",
    "        print(\"-----------------------------------------\")\n",
    "        for d in [env.best_dist for env in envs]:\n",
    "            history_sisrRL_dists.append(d)\n",
    "            \n",
    "    ranks = []\n",
    "    for i in range(n_batch):\n",
    "        ben = min(history_rand_dists[i], history_sisr2_dists[i], history_sisrRL_dists[i])\n",
    "        if ben == history_rand_dists[i]:\n",
    "            ranks.append(0)\n",
    "        elif ben == history_sisr2_dists[i]:\n",
    "            ranks.append(1)\n",
    "        else:\n",
    "            ranks.append(2)\n",
    "    ranks = np.array(ranks)\n",
    "    \n",
    "    print(\"+++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"# nodes =\", n_nodes)\n",
    "    print(\"初始化平均距离\", np.mean(history_init_dists), \"标准差\", np.std(history_init_dists))\n",
    "    print(\"随机算子平均距离\", np.mean(history_rand_dists), \"标准差\", np.std(history_rand_dists))\n",
    "    print(\"DPRR平均距离\", np.mean(history_sisr2_dists), \"标准差\", np.std(history_sisr2_dists))\n",
    "    print(\"DPRR_RL平均距离\", np.mean(history_sisrRL_dists), \"标准差\", np.std(history_sisrRL_dists))\n",
    "    print(\"(近似)最优解\", sol_dist)\n",
    "    print(\"随机算子最佳解数目\", np.sum(ranks==0))\n",
    "    print(\"DPRR最佳解数目\", np.sum(ranks==1))\n",
    "    print(\"DPRR_RL最佳解数目\", np.sum(ranks==2))\n",
    "    print(\"总耗时\", cost_time)\n",
    "    print(\"平均耗时\", cost_time/n_batch)\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear the temp files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"tmp_datas\"\n",
    "state_path = \"states.txt\"\n",
    "distMat_path = \"distMat.txt\"\n",
    "os.remove(state_path)\n",
    "os.remove(distMat_path)\n",
    "shutil.rmtree(data_root)\n",
    "_=[os.remove(filename) for filename in os.listdir() if \".class\" in filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37_torch] *",
   "language": "python",
   "name": "conda-env-py37_torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
